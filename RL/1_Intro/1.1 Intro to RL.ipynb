{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RL w Python\n",
    "\n",
    "\n",
    "\n",
    "### Methods of Reinforcement Learning\n",
    "\n",
    "__Dynamic Programming__ perfectly optimized choices for a perfectly known environment. \n",
    "\n",
    "__TD Learning__ Start with a guess of expected rewards and use trial and error to update expectations. \n",
    "\n",
    "\n",
    "### Markov Decision Processes\n",
    "__Markov__ implies the nest state only depends on the current state (not the past) We can make a problem Markov by bundling up all the information of the past along with knowledge available to the agent into a single state vector. This state might encompass all information about the environment, in which case the environment is __fully observable__.\n",
    "\n",
    "__MDPs__ Are characterized by having state, rewards, and actions.  \n",
    "\n",
    "<img src='https://miro.medium.com/max/1362/1*7cuAqjQ97x1H_sBIeAVVZg.png' width=500>\n",
    "\n",
    "### Types of MDPs\n",
    "__Bandits__ where actions taken have no effect on the environment. These are very useful in business for many things. __No Delayed rewards__ -> Since the environment isn't affected by agent's actions, there is no \"long-term planning\" involved the agent simply makes the most optimal action in each state. The environment might change on its own, but these changes cannot be anticipated as a result of the agent's decisions.\n",
    "\n",
    "__MDP/POMDP__ Agent may have full observability or partial observability of its environment.\n",
    "\n",
    "__Deterministic/Stochastic__ State change as a result of prev. state and action may be a function (same every time) or it may be random. \n",
    "\n",
    "*In the case of stochasticity, can an agent's actions change the probabilities of the state function -> Maybe?  Envokes the thought that an agent's actions can definitely change the reward function as well as the state function\n",
    "\n",
    "This is incorrect, the reward function is fixed, the state-state transition function is what changes as a result of agent action. For stochasticity this means changes in probabilities, for deterministic this means changes in the function*\n",
    "\n",
    "### RL Algorithm Components\n",
    "\n",
    "__Model__ does the agent model the dynamics of the environment. (optional) Predicts what the environment will do next, an internal representation of the environment.\n",
    "\n",
    "__Policy__ What is the rule that maps agent's state to an action (oftentimes this is a greedy rule) Probabilistic distribution of actions given states (agent's behavior function). Deterministic or stochastic\n",
    "\n",
    "__Value Function__ Given current state, what is the expected CUMULATIVe future reward for all available actions given current state.\n",
    "\n",
    "Most State of the art DL models use neural networks to approximate the value function. *I'm not sure if these architectures are model-less approaches... I also don't know what an actor-critic algorithm is.*\n",
    "\n",
    "\n",
    "\n",
    "### Types of RL Agents\n",
    "\n",
    "__Value-Based__ The value function determines utility of each action and the policy picks the best one.\n",
    "\n",
    "__Policy-Based__ explicitly represent the policy. \n",
    "\n",
    "__Actor-Critic__ agent is a value-based and a policy based agent.\n",
    "\n",
    "__Model-Based__ agent builds a model of how the environment will respond to its actions\n",
    "\n",
    "__Model-Free__ agent goes directly to policy/value function. Experience -> learn behavior\n",
    "\n",
    "\n",
    "### Off-policy vs On-Policy\n",
    "__off-policy__ learner learn the value of an optimal policy independent of the agent's actions. There are two policies, the policy used to generate behavior (agent's policy) and the estimation policy, which is evaluated and improved during learning.\n",
    "\n",
    "__On-policy__ learner learns the value of the optimal policy\n",
    "\n",
    "*From the Sutton book: \"The on-policy approach in the preceding section is actually a compromise—it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to use two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is called the behavior policy. In this case we say that learning is from data “o↵” the target policy, and the overall process is termed o↵-policy learning.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imitation Learning\n",
    "\n",
    "Don't break the problem into many sub components. Train a network to predict what the current driver will do, then learn from the difference. (In the context of self-driving cars) Use an neural network to compress the decision process of an extremely complicated (or unknown) system.\n",
    "\n",
    "#### Issues\n",
    "- Distribution Mismatch - There will be situations that the network will have never trained on. Edge cases that occur very infrequently but still require decision.\n",
    "- Error prone human behavior - Imitation can only be as good as the imitated.\n",
    "- Markov Assumption - Need to encode foresight into decisions, rather than acting solely on the present.\n",
    "\n",
    "\n",
    "### Q- Learning\n",
    "Named after the Q - function that estimates the cumulative expected reward for taking an action in a certain state.\n",
    "\n",
    "<img src='https://www.cse.unsw.edu.au/~cs9417ml/RL1/images/qalg.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "On-Policy RL algorithm for TD-Learning. In Q-Learning, the update rule uses the next action that maximizes the Q function, in SARSA, the next action is generated by the current policy.\n",
    "\n",
    "<img src='https://www.cse.unsw.edu.au/~cs9417ml/RL1/images/salg.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Learning\n",
    "Leverages a neural network to approximate the value function for each available action. The agent chooses the action with maximal output from the neural network. While training, maintain a replay memory buffer to store past experiences. Also maintain a second copy of the Q-network that will act as the \"target network\" We will use the outputs of the two networks to learn the ENV's action-value function.\n",
    "\n",
    "Note that having two networks makes the training algorithm akin to TD-Learning. Also note that having an experience buffer mitigates the effects of correlated minibatches.\n",
    "\n",
    "\n",
    "<img src='https://miro.medium.com/max/2261/1*nb61CxDTTAWR1EJnbCl1cA.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Deterministic Policy Gradient\n",
    "Relies on an actor-critic architecture. The actor tunes the parameters $\\theta$ for the policy function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Learning\n",
    "learn a task from daily activity and delegate it to the edge. Learned behavior is modified at the edge, and new behaviors are shared with the center and other edges.\n",
    "\n",
    "Consider recommendations for individual users on a mobile app. The cellular device may have its own ML model learning the best recommendation strategy, and new information can be backed up to the center. This compartmentalization at the edge also allows secure access to sensitive user data.\n",
    "\n",
    "Build a model at the center that uses enterprise data (thus enterprise data is kept close at hand) that can be applied at the edge level with transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL in Retail\n",
    "[link](https://towardsdatascience.com/deep-learning-vs-deep-reinforcement-learning-algorithms-in-retail-industry-ii-9c17c83ecf2f)\n",
    "\n",
    "RL is used to optimize assortment, stock levels, and regional prices. Supply chain maintenance to maximize efficiency, Optimizing space utilization in warehouses to reduce transit times, dynamic pricing,split delivery routing systems. \n",
    "\n",
    "#### Dynamic Pricing with RL\n",
    "Agent periodically updates prices based on env state. mode is pre-trained by historical sales data and previous human decisions. This technique is also used for training bidding agents in multiseller environments.\n",
    "\n",
    "<img src='https://miro.medium.com/max/2046/1*sjm43hZu1chZp5LJiBZhJA.png' width=600>\n",
    "\n",
    "The above image represents a single seller system. Seller maintains an inventory that is replenished at certain threshold (arrival time is exponential distr based on past replenishes) Impartial buyers and shoppers, imp buyers dont care about availability or vol. discounts. \n",
    "\n",
    "Once a buyer/shopper is offered a price they like, they move into the queue to await their item from inventory. THey can balk on an exponential distr. if they do not receive item.\n",
    "\n",
    "For multiseller systems, each captive has an associated utility function that they will attempt to maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
