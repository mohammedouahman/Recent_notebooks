{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming (Policy Iteration)\n",
    "\n",
    "We will be solving the gridworld problem with dynamic programming. The agent will be dropped in a random position on the board and find its way to one of the corners. Each turn that passes the agent will receive a reward of -1 (incentivizing finding the terminal state as quickly as possible).\n",
    "\n",
    "We will be evaluating the value functions w.r.t. random policy.  Note that the dynamics of the MDP is deterministic. Given a state and action, we know exactly what the next state will be (square above, to left, to right, or below) Likewise, the reward is known for every action, state combination.\n",
    "\n",
    "\n",
    "<img src=\"imgs\\DP_Policy_Iteration_Algorithm.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:52:08.336237Z",
     "start_time": "2019-12-05T20:52:07.764706Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "GRID_SIZE = 4\n",
    "GAMMA = 0.9\n",
    "NONTERM_REWARD = -1\n",
    "TERMINATION_STATE = [[0,0],[GRID_SIZE-1,GRID_SIZE-1]]\n",
    "ACTIONS = [[-1,0],[1,0],[0,1],[0,-1]]\n",
    "THETA = 1e-7\n",
    "ITERATION_CUTOFF = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:52:09.020194Z",
     "start_time": "2019-12-05T20:52:09.013239Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_state_reward(current_state, action):\n",
    "    if current_state in TERMINATION_STATE:\n",
    "        return current_state, 0\n",
    "    \n",
    "    next_state = np.array(current_state) + np.array(action)\n",
    "    reward = NONTERM_REWARD\n",
    "    if (-1 in next_state) or (GRID_SIZE in next_state):\n",
    "        next_state = current_state\n",
    "    return next_state, reward\n",
    "\n",
    "def get_reward(current_state, action):\n",
    "    _, reward = get_state_reward(current_state, action)\n",
    "    return reward\n",
    "\n",
    "def get_state(current_state, action):\n",
    "    state, _ = get_state_reward(current_state, action)\n",
    "    return state\n",
    "\n",
    "def get_value(next_state, val_map):\n",
    "    return val_map[next_state[0], next_state[1]]\n",
    "    \n",
    "def set_value(value, state, val_map):\n",
    "    val_map[state[0], state[1]] = value\n",
    "    return val_map\n",
    "    \n",
    "def initialize_state_values():\n",
    "    \"\"\"Simply initialize the value matrix and the state index\n",
    "    \"\"\"\n",
    "    value_map = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    states = [[i, j] for i in range(GRID_SIZE) for j in range(GRID_SIZE)]\n",
    "    return states, value_map\n",
    "\n",
    "def random_policy(current_state, value_map=None):\n",
    "    return [1/len(ACTIONS) for i in range(len(ACTIONS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:52:47.142058Z",
     "start_time": "2019-12-05T20:52:47.132085Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, stop_delta=True, val_map = None, _print=True, ret_deltas=True):\n",
    "    iteration = 0\n",
    "    deltas = [] \n",
    "    delta = 0\n",
    "    if val_map is None:\n",
    "        states, value_map = initialize_state_values()\n",
    "    else:\n",
    "        states, _ = initialize_state_values()\n",
    "        value_map = val_map\n",
    "    \n",
    "    stop_delta_flag = True\n",
    "    while (stop_delta_flag):\n",
    "        delta = 0\n",
    "        if iteration > ITERATION_CUTOFF:\n",
    "            break\n",
    "        else:\n",
    "            next_value_map = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "            for state in states:\n",
    "                v = get_value(state, value_map)\n",
    "                updated_v = 0\n",
    "                \n",
    "                action_probabilities = policy(state, value_map=value_map)\n",
    "\n",
    "                for i, action in enumerate(ACTIONS):\n",
    "                    next_state, reward = get_state_reward(state, action)\n",
    "                    updated_v += (action_probabilities[i])*(reward + (GAMMA*get_value(next_state, value_map)))\n",
    "                    \n",
    "                delta = max([delta, abs(v - updated_v)])\n",
    "                next_value_map = set_value(updated_v, state, next_value_map)\n",
    "                \n",
    "            deltas.append(delta)\n",
    "            value_map = next_value_map\n",
    "            iteration+=1\n",
    "            if _print and iteration in [1,2,1000]:\n",
    "                print(\"Iteration {}: \\nDelta: {}\".format(iteration, round(delta,3)))\n",
    "                print(value_map.round(3))\n",
    "                print()\n",
    "        if stop_delta:\n",
    "            stop_delta_flag = THETA < delta\n",
    "    if _print:        \n",
    "        print(\"Iteration {}: \\nDelta: {}\".format(iteration, round(delta,3)))\n",
    "        print(value_map.round(2))\n",
    "        print()\n",
    "    \n",
    "    if ret_deltas:\n",
    "        res = (value_map, deltas)\n",
    "    else:\n",
    "        res = value_map\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:52:47.803896Z",
     "start_time": "2019-12-05T20:52:47.653406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "Policy Evaluation on Grid World\n",
      "Iteration 1: \n",
      "Delta: 1.0\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "Iteration 2: \n",
      "Delta: 0.9\n",
      "[[ 0.    -1.675 -1.9   -1.9  ]\n",
      " [-1.675 -1.9   -1.9   -1.9  ]\n",
      " [-1.9   -1.9   -1.9   -1.675]\n",
      " [-1.9   -1.9   -1.675  0.   ]]\n",
      "\n",
      "Iteration 103: \n",
      "Delta: 0.0\n",
      "[[ 0.   -5.28 -7.13 -7.65]\n",
      " [-5.28 -6.61 -7.18 -7.13]\n",
      " [-7.13 -7.18 -6.61 -5.28]\n",
      " [-7.65 -7.13 -5.28  0.  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Policy Evaluation on Grid World\")\n",
    "final_val_map_random, delta_random = policy_evaluation(random_policy, stop_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:52:53.878455Z",
     "start_time": "2019-12-05T20:52:53.644499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Random Policy Convergence\")\n",
    "plt.plot(range(len(delta_random)), delta_random, label= \"Random Policy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement Theorem\n",
    "\n",
    "There is a partial ordering among policies that we can use to compare two policies. Given to policies pi and pi', pi' is a better is as good or better than pi if for all states of the MDP:\n",
    "<img src=\"imgs\\policy_improvement_theorem2.PNG\">\n",
    "\n",
    "\n",
    "\n",
    "One way a policy can be improved is to create a new policy by performing greedy selection on the previous policies value function. We can prove that this new greedy policy is at least as good as the old policy by checking if for each state.\n",
    "<img src=\"imgs\\policy_improvement_theorem.PNG\">\n",
    "\n",
    "In the below code, we test whether applying greedy selection to the value function for the random policy will result in a new policy that is strictly better for all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:53:13.976750Z",
     "start_time": "2019-12-05T20:53:13.966691Z"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_policy(current_state, value_map=None):\n",
    "    \"\"\"Returns the probability of next actions. (1 if no tie)\"\"\"\n",
    "    possible_rewards = [get_reward(current_state, action)+(GAMMA*get_value(get_state(current_state, action), value_map)) for action in ACTIONS]\n",
    "    max_r = max(possible_rewards)\n",
    "    greedy_actions = [1/(np.array(possible_rewards)==max_r).sum() if r==max_r else 0 for r in possible_rewards]\n",
    "    return greedy_actions\n",
    "\n",
    "def did_policy_improve(state, value_map, new_policy):\n",
    "    action_probabilities = new_policy(state, value_map)\n",
    "    q_pi = 0 \n",
    "    for i, action in enumerate(ACTIONS):\n",
    "        next_state, reward = get_state_reward(state, action)\n",
    "        q_pi += (action_probabilities[i])*(reward + (GAMMA*get_value(next_state, value_map)))\n",
    "    return q_pi >= get_value(state, value_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:53:15.451896Z",
     "start_time": "2019-12-05T20:53:15.428916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "Policy Improvement\n",
      "Is greedy selection better \n",
      "in the current state? \n",
      "\n",
      "[True, True, True, True]\n",
      "[True, True, True, True]\n",
      "[True, True, True, True]\n",
      "[True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Policy Improvement\")\n",
    "states, _ = initialize_state_values()\n",
    "result = [[i for i in range(GRID_SIZE)] for j in range(GRID_SIZE)]\n",
    "for state in states:\n",
    "    improved = did_policy_improve(state, final_val_map_random, greedy_policy)\n",
    "    result[state[0]][state[1]] = improved\n",
    "\n",
    "print(\"Is greedy selection better \\nin the current state?\", '\\n')    \n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:53:37.063511Z",
     "start_time": "2019-12-05T20:53:32.931542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function:\n",
      "[[ 0.   -1.   -1.9  -2.71]\n",
      " [-1.   -1.9  -2.71 -1.9 ]\n",
      " [-1.9  -2.71 -1.9  -1.  ]\n",
      " [-2.71 -1.9  -1.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "VAL_MAP = final_val_map_random\n",
    "for i in range(1000):\n",
    "    VAL_MAP = policy_evaluation(greedy_policy, stop_delta=True, val_map=VAL_MAP, _print=False, ret_deltas=False)\n",
    "print(\"Optimal Value Function:\")\n",
    "print(VAL_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:53:50.502602Z",
     "start_time": "2019-12-05T20:53:50.483454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "['UDRL', 'L', 'L', 'DL']\n",
      "['U', 'UL', 'UDRL', 'D']\n",
      "['U', 'UDRL', 'DR', 'D']\n",
      "['UR', 'R', 'R', 'UDRL']\n"
     ]
    }
   ],
   "source": [
    "def read_action(actions):\n",
    "    res = ''\n",
    "    names = ['U','D','R','L']\n",
    "    for i, act in enumerate(actions):\n",
    "        if act>0:\n",
    "            res+=names[i]\n",
    "    return res\n",
    "\n",
    "states, _ = initialize_state_values()\n",
    "optimal_actions = [[i for i in range(GRID_SIZE)] for j in range(GRID_SIZE)]\n",
    "for state in states:\n",
    "    action = greedy_policy(state, value_map=VAL_MAP)\n",
    "    optimal_actions[state[0]][state[1]] = read_action(action)\n",
    "print('Optimal Policy:')\n",
    "for row in optimal_actions:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "<img src=\"imgs\\Q_Learning_algorithm.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:54:13.544234Z",
     "start_time": "2019-12-05T20:54:13.540284Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:54:51.735743Z",
     "start_time": "2019-12-05T20:54:51.700798Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_terminations(grid_size, num_holes=2):\n",
    "    termination_states = []\n",
    "    while len(termination_states)<=num_holes:\n",
    "        hole = [randint(0,grid_size-1),randint(0,grid_size-1)]\n",
    "        if hole not in termination_states:\n",
    "            termination_states.append(hole)\n",
    "    return termination_states\n",
    "\n",
    "def get_state_reward(current_state, action):\n",
    "    rando = random.random()\n",
    "    if rando<SLIP_PROB: #slip\n",
    "        next_state = np.array(current_state) + 2*np.array(action)\n",
    "        if next_state[0]<=-1:\n",
    "            next_state[0] = 0\n",
    "        if next_state[1]<=-1:\n",
    "            next_state[1] = 0\n",
    "        if next_state[0]>=GRID_SIZE:\n",
    "            next_state[0] = GRID_SIZE - 1\n",
    "        if next_state[1]>=GRID_SIZE:\n",
    "            next_state[1] = GRID_SIZE - 1\n",
    "        if list(np.array(current_state) + np.array(action)) in TERMINATION_STATE:\n",
    "            return list(next_state), TERM_REWARD #slipped into hole\n",
    "    else:\n",
    "        next_state = np.array(current_state) + np.array(action)\n",
    "\n",
    "    next_state = list(next_state)\n",
    "    if next_state in TERMINATION_STATE:\n",
    "        return next_state, TERM_REWARD #hole\n",
    "    else:\n",
    "        return next_state, NONTERM_REWARD\n",
    "\n",
    "def possible_actions(state):\n",
    "    actions =[]\n",
    "    if state[0]!=0:\n",
    "        actions.append([-1,0])\n",
    "    if state[1]!=0:\n",
    "        actions.append([0,-1]) \n",
    "    if state[0]!=GRID_SIZE-1:\n",
    "        actions.append([1,0])\n",
    "    if state[1]!=GRID_SIZE-1:\n",
    "        actions.append([0,1])\n",
    "    return actions\n",
    "\n",
    "    \n",
    "def initialize_Q_matrix():\n",
    "    Q_matrix = dict()\n",
    "    states = [[i, j] for i in range(GRID_SIZE) for j in range(GRID_SIZE)]\n",
    "    for state in states:\n",
    "        actions = possible_actions(state)\n",
    "        for action in actions:\n",
    "            Q_matrix[str(state[0])+str(state[1])+str(action)] = random.random()\n",
    "    return Q_matrix\n",
    "\n",
    "\n",
    "def greedy_policy(current_state, Q_matrix=None, epsilon=0.05):\n",
    "    \"\"\"Returns the probability of next actions. (1 if no tie)\"\"\"\n",
    "    actions = possible_actions(current_state)\n",
    "    rando = random.random()\n",
    "    if rando<epsilon:#random exploration\n",
    "        return random.choice(actions)\n",
    "    possible_rewards = [Q_matrix[str(current_state[0])+str(current_state[1])+str(action)] for action in actions]\n",
    "    i, max_r = max(enumerate(possible_rewards), key=lambda x: x[1])\n",
    "    greedy_action = actions[i]\n",
    "    return greedy_action\n",
    "\n",
    "\n",
    "def read_action(actions):\n",
    "    names = ['U','D','R','L']\n",
    "    if action ==[0,1]:\n",
    "        return 'R'\n",
    "    if action ==[1,0]:\n",
    "        return 'D'\n",
    "    if action ==[0,-1]:\n",
    "        return 'L'\n",
    "    if action ==[-1,0]:\n",
    "        return 'U'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:54:57.137862Z",
     "start_time": "2019-12-05T20:54:57.130846Z"
    }
   },
   "outputs": [],
   "source": [
    "def Q_Learn(num_iterations=1000):\n",
    "    iteration = 0\n",
    "    states = [[i, j] for i in range(GRID_SIZE) for j in range(GRID_SIZE)]\n",
    "    \n",
    "    Q_matrix = initialize_Q_matrix()\n",
    "    while (iteration<num_iterations):\n",
    "        iteration+=1\n",
    "        epsilon = 1 - iteration/(num_iterations/2) #agent explores at start\n",
    "        current_state = random.choice(states)\n",
    "        reward = 0\n",
    "        moves=0\n",
    "        while reward!=TERM_REWARD:\n",
    "            if moves>2000:\n",
    "                break\n",
    "            action = greedy_policy(current_state, Q_matrix=Q_matrix, epsilon=epsilon)\n",
    "            next_state, reward = get_state_reward(current_state, action)\n",
    "            previous_Q = Q_matrix[str(current_state[0])+str(current_state[1])+str(action)]\n",
    "            next_Q = Q_matrix[str(next_state[0])+str(next_state[1])+str(greedy_policy(next_state, Q_matrix=Q_matrix, epsilon=epsilon))]\n",
    "            update = (1-ALPHA)*previous_Q + ALPHA*(reward+GAMMA*next_Q)\n",
    "            Q_matrix[str(current_state[0])+str(current_state[1])+str(action)] = update\n",
    "            current_state = next_state\n",
    "            moves+=1\n",
    "    return Q_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:57:17.320812Z",
     "start_time": "2019-12-05T20:56:42.155460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "Q-Learning on Ice Skating Rink\n",
      "Rink:\n",
      "[' ', ' ', ' ', ' ', ' ']\n",
      "['X', 'X', ' ', ' ', ' ']\n",
      "[' ', ' ', ' ', ' ', 'X']\n",
      "[' ', ' ', ' ', ' ', ' ']\n",
      "[' ', ' ', ' ', ' ', ' ']\n",
      "Optimal Policy:\n",
      "['R', 'R', 'R', 'D', 'L']\n",
      "['X', 'X', 'R', 'U', 'U']\n",
      "['D', 'D', 'D', 'D', 'X']\n",
      "['D', 'R', 'D', 'D', 'L']\n",
      "['R', 'R', 'L', 'L', 'L']\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Q-Learning on Ice Skating Rink\")\n",
    "\n",
    "GRID_SIZE = 5\n",
    "TERMINATION_STATE = random_terminations(GRID_SIZE, num_holes=2) \n",
    "SLIP_PROB = 0.2\n",
    "\n",
    "NONTERM_REWARD = 100\n",
    "TERM_REWARD = -1000\n",
    "\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.3\n",
    "\n",
    "states = [[i, j] for i in range(GRID_SIZE) for j in range(GRID_SIZE)]\n",
    "rink = [[i for i in range(GRID_SIZE)] for j in range(GRID_SIZE)]\n",
    "for state in states:\n",
    "    if state in TERMINATION_STATE:\n",
    "        rink[state[0]][state[1]] = 'X'\n",
    "    else:\n",
    "        rink[state[0]][state[1]] = ' '\n",
    "\n",
    "print(\"Rink:\")\n",
    "for row in rink:\n",
    "    print(row)\n",
    "\n",
    "Q_matrix = Q_Learn(num_iterations=500)\n",
    "\n",
    "optimal_action = [[i for i in range(GRID_SIZE)] for j in range(GRID_SIZE)]\n",
    "for state in states:\n",
    "    action = greedy_policy(state, Q_matrix=Q_matrix, epsilon=0)\n",
    "    if state in TERMINATION_STATE:\n",
    "        optimal_action[state[0]][state[1]] = 'X'\n",
    "    else:\n",
    "        optimal_action[state[0]][state[1]] = read_action(action)\n",
    "print('Optimal Policy:')\n",
    "for row in optimal_action:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
