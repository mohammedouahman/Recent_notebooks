{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "nums=sc.parallelize([1,2,3,4]) #RDD parallelized across cluster\n",
    "nums.take(1)#get first row\n",
    "nums.map(lambda x: x*x).collect() #map function to data\n",
    "\n",
    "\n",
    "#Create a Spark DataFrame\n",
    "import pyspark.sql as psql\n",
    "sqlContext = psql.SQLContext(sc)\n",
    "\n",
    "#create a DataFrame context\n",
    "list_p = [('John',19),('Smith',29),('Adam',35),('Henry',50)]\n",
    "rdd=sc.parallelize(list_p)\n",
    "ppl = rdd.map(lambda x: psql.Row(name=x[0], age=x[1]))\n",
    "DF_ppl = sqlContext.createDataFrame(ppl)\n",
    "\n",
    "#to access the type of each feature\n",
    "DF_ppl.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BASIC OPERATIONS WITH PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#read the csv file with InfetSchema=True\n",
    "df = sqlContext.read.csv(SparkFiles.get(\"adult_data.csv\"),header=True,inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recasting the columns to a different format\n",
    "#use withColumn to apply a transformation to one column\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names:\n",
    "        df = df.withColumn(name,df[name].cast(newType))\n",
    "    return df\n",
    "\n",
    "Continuous_Features = ['age', 'fnlwgt','capital_gain', 'education_num', 'capital_loss', 'hours_week']\n",
    "#convert type of above columns\n",
    "df = convertColumn(df,Continuous_Features. FloatType())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select and show the names of the features\n",
    "df.select('age','fnlwgt').show(5)\n",
    "\n",
    "#Count the number of occurences by group\n",
    "df.groupBy('age').count().sort('count', ascending=True).show(5)\n",
    "\n",
    "#get summary statistics withdescribe()\n",
    "df.describe().show()\n",
    "df.describe('capital_gain').show()\n",
    "\n",
    "#drop column\n",
    "df.drop('education_num')\n",
    "#access the columns \n",
    "df.columns\n",
    "#fillna\n",
    "df.fillna(0)\n",
    "\n",
    "#filter data \n",
    "df.filter(df.age>40).show(5)\n",
    "\n",
    "#groupBy and Aggregation\n",
    "df.groupBy(\"marital\").agg({'capital_gain':'mean'}).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.function import *\n",
    "\n",
    "#select the column ??WHY??\n",
    "age_square = df.select(col(\"age\")**2) #????????\n",
    "#apply transformation and add it to the dataframe\n",
    "df = df.withColumn('age_square',col(\"age\")**2)\n",
    "\n",
    "#change the order of columns with select\n",
    "#cols = [colname1, colname2,...]\n",
    "#df = df.selects(cols)\n",
    "\n",
    "#remove entries with a given criteria\n",
    "df = df.filter(df.native_country!=\"Holand-Netherlands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "#OHE encoding a column of strings\n",
    "#index the string col to numeric\n",
    "stringIndexer = StringIndexer(inputCol='workclass',outputCol='workclass_encoded')\n",
    "model = stringIndexer.fit(df)\n",
    "df = model.transform(df)\n",
    "\n",
    "#OneHotEncode the numeric column\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol='workclass_encoded', outputCol='workclass_vec')\n",
    "df = encoder.transform(df)\n",
    "df.show(2)\n",
    "\n",
    "\n",
    "#BUILD THE PIPELINE\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "Continuous_Features = ['age', 'fnlwgt','capital_gain', 'education_num', 'capital_loss', 'hours_week']\n",
    "stages = [] #stages to be addded to the Pipeline\n",
    "\n",
    "#loop to create a OHE encoder for each categorical variable\n",
    "for categorical_col in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                    ouputCols=[categoricalCol_+'classVec'])\n",
    "    stages+=[stringIndexer, encoder]\n",
    "    #This OHE for a column is one stage\n",
    "\n",
    "#OHE the target_labels\n",
    "label_stringIDX = StringIndexer(inputCol='label', outputCol='newlabel')\n",
    "stages+=[label_stringIDX]    \n",
    "\n",
    "#Combine the features into one matrix\n",
    "assembler_inputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\") \n",
    "stages+=[assembler]\n",
    "\n",
    "#Note that the stages variable is a list of stages (grouped in lists)\n",
    "\n",
    "#Push to Pipeline\n",
    "pipeline=Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "model = pipelineModel.transform(df)\n",
    "model.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mi.linalg import DenseVectore\n",
    "input_data = model.rdd.map(lambda x: (x['newlabel'], DenseVector(x['features'])))\n",
    "\n",
    "#create a DataFrame\n",
    "df_train = sqlContect.createDataFrame(input_data,['label', 'features'])\n",
    "df_train.show(5)\n",
    "\n",
    "#train/test split\n",
    "train, test = df_train.randomSplit([.8,.2],seed=1234)\n",
    "train.groupBy('label').agg({'label':'count'}).show()\n",
    "\n",
    "#Build Logistic Regressor\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol='label',\n",
    "                       featuresCol='features',\n",
    "                       maxIter=20,\n",
    "                       regParams=0.1)\n",
    "lr = lr.fit(train)\n",
    "\n",
    "#print coefficients\n",
    "print(\"Coefficients: \" + str(lr.coefficients))\n",
    "print(str(lr.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = linearModel.transform(test_data)\n",
    "preds.printSchema() #includes true label, feats, probs, and preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.select('label','prediction','probability')\n",
    "#accuracy\n",
    "preds.filter(preds.label==preds.prediction).count()/preds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "#create a Parameter Grid\n",
    "paramGrid = (ParamGridBuilder().addGrid(lr.regParam,[0.01, 0.5]).build())\n",
    "\n",
    "#create a 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                   estimatorParamMaps=paramGrid,\n",
    "                   evaluator=evaluator,\n",
    "                   numFolds=5)\n",
    "\n",
    "model = cv.fit(train_data)\n",
    "preds = model.transform(test)\n",
    "preds = preds.select('label','prediction','probability')\n",
    "#accuracy\n",
    "preds.filter(preds.label==preds.prediction).count()/preds.count()\n",
    "\n",
    "\n",
    "#extract the recommended best Parameter\n",
    "model.bestModel.extractParamMap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
