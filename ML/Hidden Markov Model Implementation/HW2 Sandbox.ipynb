{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## HMM POS\n",
    "HMM is a sequence model, assigns a label to each unit in a sequence. Maps sequence of obsvs to a sequence of labels. It is a probablistic model, given a sequence it computes a probability distr over possible sequence of labels and chooses the label sequence with the highest probability.\n",
    "\n",
    "### Markov Chains\n",
    "Tells us something about the probabilities of the sequence of random variables (states). Assumes that future predictions only depend on the current state, and not the past.\n",
    "\n",
    "Markov Assumption: P(qi = a|q1...qi−1) = P(qi = a|qi−1) \n",
    "\n",
    "The value of transition probabilities from a state must sum to one\n",
    "\n",
    "### Hidden Markov Model\n",
    "Markov chains are used when the sequence is observable. HMM predict sequences that are hidden (unobserved)\n",
    "HMM allows us to talk about observed events and hidden events which we think of as causal factors in the model.\n",
    "\n",
    "Q = set of states. A = transition probability matrix (aij is prob oving i to j)\n",
    "O = sequence of observations. B = sequence of observation likelihoods expressing prob of oi being generated from a state qi, PI = initial prob distr of starting states.\n",
    "\n",
    "Output Independence: P(oi\n",
    "|q1 ...qi\n",
    ",...,qT ,o1,...,oi\n",
    ",...,oT ) = P(oi\n",
    "|qi)\n",
    "\n",
    "HMM has A and B probability components. A matrix contains the tag transition probs. prob of a tag occuring after prev tag.\n",
    "\n",
    "<img src=imgs/Amatrix.png>\n",
    "\n",
    "B emision probs P(wi|ti) repr the prob given a tag, it wil be associated with a given word.\n",
    "\n",
    "<img src=imgs/Bmatrix.png>\n",
    "\n",
    "<img src=imgs/assum.png>\n",
    "\n",
    "<img src=imgs/final.png>\n",
    "\n",
    "\n",
    "\n",
    "# Vertibi Algorithm\n",
    "Decoding algorithm for HMMs is the Viterbi Algorithm. Start with a prob lattice of with N columns (n is num words in sentence) and t rows. (t is the number of states in the state graph... POS)\n",
    "\n",
    "Each cell of the lattice v(j) represents the probability that the HMM is in state j after seeing the first t observations and passing through the most probable state sequence q1....qt-1. At each cell, recursively compute the most probable path that would lead to that cell.\n",
    "\n",
    "<img src=imgs/vit.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "\n",
    "class Tagger:\n",
    "    def __init__(self):\n",
    "        self.initial_tag_probability = None\n",
    "        self.transition_probability = None\n",
    "        self.emission_probability = None\n",
    "        self.suffix_probability = None\n",
    "\n",
    "    def load_corpus(self, path):\n",
    "        if not os.path.isdir(path):\n",
    "            sys.exit(\"Input path is not a directory\")\n",
    "        for filename in os.listdir(path):\n",
    "            filename = os.path.join(path, filename)\n",
    "            try:\n",
    "                reader = io.open(filename)\n",
    "                data = reader.readlines()\n",
    "                \n",
    "                #remove header and footer\n",
    "                data = data[2:-2]\n",
    "\n",
    "                #replace utf-16 formatting\n",
    "                #data = [sentence.replace(\"\\n\",'') for sentence in data]\n",
    "                data = [sentence.replace(\"\\n\",'') for sentence in data]\n",
    "                data = [sentence.replace(\"\\00\",'') for sentence in data]\n",
    "\n",
    "                #split sentence into list on spaces\n",
    "                data = [sentence.split() for sentence in data]\n",
    "\n",
    "                output = []\n",
    "                for innerlist in data:\n",
    "                    #skip sentences of len < 2\n",
    "                    if len(innerlist)<3:\n",
    "                        continue\n",
    "                    inneroutput = []\n",
    "                    for pair in innerlist:\n",
    "                        pair = pair.split('/')\n",
    "                        #remove PAIRs that dont split in two\n",
    "                        if len(pair)!=2:\n",
    "                            continue\n",
    "                        #Getting rid of these suckers\n",
    "                        if pair[0]=='brown_modified':\n",
    "                            continue\n",
    "                        else:\n",
    "                            inneroutput.append( (pair[0].lower(),pair[1]) )\n",
    "                    #dont include sentences with 1 or fewer pairs\n",
    "                    if len(inneroutput)<2:\n",
    "                        continue\n",
    "                    output.append(inneroutput)  \n",
    "                \n",
    "                return output\n",
    "            except IOError:\n",
    "                sys.exit(\"Cannot read file\")\n",
    "\n",
    "    def initialize_probabilities(self, sentences):\n",
    "        if type(sentences) != list:\n",
    "            sys.exit(\"Incorrect input to method\")\n",
    "        \n",
    "        def ifin_add(var, dic):\n",
    "            #Helper function to count\n",
    "            if var in dic:\n",
    "                dic[var] += 1\n",
    "            else:\n",
    "                dic[var] = 1\n",
    "\n",
    "        def ifin_div(var, dic, divisor, v_size):\n",
    "            #Helper function to divide WITH SMOOTHING\n",
    "            if var in dic:\n",
    "                dic[var] = (dic[var] + 1) / (divisor + v_size)\n",
    "            else:\n",
    "                dic[var] = 1 / (divisor + v_size)\n",
    "\n",
    "        TAG = {}\n",
    "        WORD = {}\n",
    "        TAG2 = {}\n",
    "        INIT = {}\n",
    "        SUFFIX = {}\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence)):\n",
    "                word, tag = sentence[i][0], sentence[i][1]\n",
    "                #count initial tag of sentence\n",
    "                if i==0:\n",
    "                    ifin_add(tag, INIT)\n",
    "\n",
    "                #Tag counts for all sentences C(ti)\n",
    "                ifin_add(tag, TAG)\n",
    "\n",
    "                #Tag_counts for word-tag C(ti,wi)\n",
    "                if word not in WORD:\n",
    "                    WORD[word]={}\n",
    "                ifin_add(tag, WORD[word])\n",
    "\n",
    "                suffix = word[-2:]\n",
    "                #Tag_counts for suffix-tag C(ti,wi-2)\n",
    "                if suffix not in SUFFIX:\n",
    "                    SUFFIX[suffix]={}\n",
    "                ifin_add(tag, SUFFIX[suffix])\n",
    "\n",
    "                #tag-tag counts C(ti,ti-1)\n",
    "                if i > 0:\n",
    "                    to_tag = tag\n",
    "                    from_tag = sentence[i-1][1]\n",
    "                    if from_tag not in TAG2:\n",
    "                        TAG2[from_tag]={}\n",
    "                    ifin_add(to_tag, TAG2[from_tag])\n",
    "        \n",
    "        #Convert counts to probabilities\n",
    "        tags = list(TAG.keys())\n",
    "        words = list(WORD.keys())\n",
    "        suffixes = list(SUFFIX.keys())\n",
    "        vocab_size = len(words)\n",
    "        for tag in tags:\n",
    "            #initial probabilities\n",
    "            ifin_div(tag, INIT, len(sentences), vocab_size)\n",
    "\n",
    "            #emission probabilities\n",
    "            for word in words:\n",
    "                ifin_div(tag, WORD[word], TAG[tag], vocab_size)\n",
    "            #suffix probabilities\n",
    "            for suff in suffixes:\n",
    "                ifin_div(tag, SUFFIX[suff], TAG[tag], len(suffixes))\n",
    "\n",
    "            #transition probabilities\n",
    "            for to_tag in tags:\n",
    "                ifin_div(to_tag, TAG2[tag], TAG[tag], vocab_size)       \n",
    "        \n",
    "        self.initial_tag_probability = INIT\n",
    "        self.transition_probability = TAG2\n",
    "        self.emission_probability = WORD\n",
    "        self.suffix_probability = SUFFIX\n",
    "        return\n",
    "\n",
    "    def viterbi_decode(self, sentence):\n",
    "        if type(sentence) != str:\n",
    "            sys.exit(\"Incorrect input to method\")\n",
    "            \n",
    "        sentence = sentence.lower().split()\n",
    "\n",
    "        tags = list(self.initial_tag_probability.keys())\n",
    "        \n",
    "        #List of dictionaries so i dont have to remember which POS is which index\n",
    "        viterb = [{tag:j for (tag,j) in zip(tags,range(len(tags)))} for i in range(len(sentence))]\n",
    "        backp = [{tag:j for (tag,j) in zip(tags,range(len(tags)))} for i in range(len(sentence))]\n",
    "\n",
    "        #Calculate the self.initial_tag_probabilityial probabilities\n",
    "        for tag in tags:\n",
    "            if sentence[0] in self.emission_probability:\n",
    "                viterb[0][tag] = self.initial_tag_probability[tag]*self.emission_probability[sentence[0]][tag]\n",
    "            else:\n",
    "                viterb[0][tag] = 0.0\n",
    "            backp[0][tag] = 0\n",
    "\n",
    "        #Calculating the lattice    \n",
    "        for t in range(1, len(sentence)):\n",
    "            for tag in tags:\n",
    "                #The backwards probability for POS = P(t-1,s')*P(s'->s)*P(w,s)\n",
    "                if sentence[t] in WORD:\n",
    "                    vals = {prev_tag : viterb[t-1][prev_tag] * TAG2[prev_tag][tag] * WORD[sentence[t]][tag] for prev_tag in tags}\n",
    "                else:\n",
    "                    vals = {prev_tag : viterb[t-1][prev_tag] * TAG2[prev_tag][tag] * SUFFIX[sentence[t][-2:]][tag] for prev_tag in tags}\n",
    "                viterb[t][tag] = vals[max(vals, key=vals.get)]\n",
    "                backp[t][tag] = max(vals, key=vals.get)\n",
    "        \n",
    "        #extracting highest value POS\n",
    "        prob_seq = []\n",
    "        t = len(sentence)-1\n",
    "        best_backpath = max(viterb[t], key=viterb[t].get)\n",
    "        prob_seq.append(best_backpath)\n",
    "        for i in range(0, len(sentence)-1):\n",
    "            t=len(sentence)-i-1\n",
    "            best_backpath = backp[t][best_backpath]\n",
    "            prob_seq.append(best_backpath)\n",
    "        return prob_seq[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'VERB',\n",
       " 'X',\n",
       " 'VERB',\n",
       " 'DETERMINER',\n",
       " 'NOUN',\n",
       " 'PREPOSITION',\n",
       " 'DETERMINER',\n",
       " 'NOUN',\n",
       " 'PREPOSITION',\n",
       " 'ADJECTIVE',\n",
       " 'NOUN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\harri\\\\Desktop\\\\NLP Assignment 2\\\\brown_modified_pos'\n",
    "tag = Tagger()\n",
    "sentences = tag.load_corpus(path)\n",
    "tag.initialize_probabilities(sentences)\n",
    "sentence = \"People continue to enquire the reason for the race for outer space .\"\n",
    "tag.viterbi_decode(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('finally', 'ADVERB'), (',', 'PUNCT'), ('it', 'PRONOUN'), ('did', 'VERB'), ('seem', 'VERB'), ('clear', 'ADJECTIVE'), ('as', 'CONJUNCTION'), ('day', 'NOUN'), ('to', 'PREPOSITION'), ('these', 'DETERMINER'), ('clergymen', 'NOUN'), (',', 'PUNCT'), ('as', 'CONJUNCTION'), (\"gannett's\", 'NOUN'), ('son', 'NOUN'), ('explained', 'VERB'), ('in', 'PREPOSITION'), ('the', 'DETERMINER'), ('biography', 'NOUN'), ('of', 'PREPOSITION'), ('his', 'PRONOUN'), ('father', 'NOUN'), (',', 'PUNCT'), ('they', 'PRONOUN'), ('had', 'VERB'), ('always', 'ADVERB'), ('contended', 'VERB'), ('for', 'PREPOSITION'), ('the', 'DETERMINER'), ('propriety', 'NOUN'), ('of', 'PREPOSITION'), ('their', 'PRONOUN'), ('claim', 'NOUN'), ('to', 'PREPOSITION'), ('the', 'DETERMINER'), ('title', 'NOUN'), ('of', 'PREPOSITION'), ('christians', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\harri\\\\Desktop\\\\NLP Assignment 2\\\\brown_modified_pos\\\\brown_modified_pos'\n",
    "reader = io.open(path, encoding='utf-8')\n",
    "data = reader.readlines()\n",
    "#remove header and footer\n",
    "data = data[2:-2]\n",
    "\n",
    "#replace utf-16 formatting\n",
    "data = [sentence.replace(\"\\n\",'') for sentence in data]\n",
    "data = [sentence.replace(\"\\00\",'') for sentence in data]\n",
    "\n",
    "\n",
    "#split sentence into list on spaces\n",
    "data = [sentence.split() for sentence in data]\n",
    "\n",
    "output = []\n",
    "for innerlist in data:\n",
    "    #skip sentences of len < 2\n",
    "    if len(innerlist)<3:\n",
    "        continue\n",
    "    inneroutput = []\n",
    "    for pair in innerlist:\n",
    "        pair = pair.split('/')\n",
    "        #remove PAIRs that dont split in two\n",
    "        if len(pair)!=2:\n",
    "            continue\n",
    "        if pair[0]=='brown_modified':\n",
    "            continue\n",
    "        else:\n",
    "            inneroutput.append( (pair[0].lower(),pair[1]) )\n",
    "    #dont include sentences with 1 or fewer pairs\n",
    "    if len(inneroutput)<2:\n",
    "        continue\n",
    "    output.append(inneroutput)    \n",
    "print(output[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'tacos'[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = output\n",
    "def ifin_add(var, dic):\n",
    "    #Helper function to count\n",
    "    if var in dic:\n",
    "        dic[var] += 1\n",
    "    else:\n",
    "        dic[var] = 1\n",
    "\n",
    "def ifin_div(var, dic, divisor, v_size):\n",
    "    #Helper function to divide WITH SMOOTHING\n",
    "    if var in dic:\n",
    "        dic[var] = (dic[var] + 1) / (divisor + v_size)\n",
    "    else:\n",
    "        dic[var] = 1 / (divisor + v_size)\n",
    "        \n",
    "TAG = {}\n",
    "WORD = {}\n",
    "TAG2 = {}\n",
    "INIT = {}\n",
    "SUFFIX = {}\n",
    "for sentence in sentences:\n",
    "    for i in range(len(sentence)):\n",
    "        word, tag = sentence[i][0], sentence[i][1]\n",
    "        #count initial tag of sentence\n",
    "        if i==0:\n",
    "            ifin_add(tag, INIT)\n",
    "        \n",
    "        #Tag counts for all sentences C(ti)\n",
    "        ifin_add(tag, TAG)\n",
    "        \n",
    "        #Tag_counts for word-tag C(ti,wi)\n",
    "        if word not in WORD:\n",
    "            WORD[word]={}\n",
    "        ifin_add(tag, WORD[word])\n",
    "        \n",
    "        suffix = word[-2:]\n",
    "        #Tag_counts for suffix-tag C(ti,wi-2)\n",
    "        if suffix not in SUFFIX:\n",
    "            SUFFIX[suffix]={}\n",
    "        ifin_add(tag, SUFFIX[suffix])\n",
    "        \n",
    "        #tag-tag counts C(ti,ti-1)\n",
    "        if i > 0:\n",
    "            to_tag = tag\n",
    "            from_tag = sentence[i-1][1]\n",
    "            if from_tag not in TAG2:\n",
    "                TAG2[from_tag]={}\n",
    "            ifin_add(to_tag, TAG2[from_tag])\n",
    "\n",
    "tags = list(TAG.keys())\n",
    "words = list(WORD.keys())\n",
    "suffixes = list(SUFFIX.keys())\n",
    "vocab_size = len(words)\n",
    "#Convert counts to probabilities\n",
    "for tag in tags:\n",
    "    #initial probabilities\n",
    "    ifin_div(tag, INIT, len(sentences), vocab_size)\n",
    "        \n",
    "    #emission probabilities\n",
    "    for word in words:\n",
    "        ifin_div(tag, WORD[word], TAG[tag], vocab_size)\n",
    "        \n",
    "    for suff in suffixes:\n",
    "        ifin_div(tag, SUFFIX[suff], TAG[tag], len(suffixes))\n",
    "    \n",
    "    #transition probabilities\n",
    "    for to_tag in tags:\n",
    "        ifin_div(to_tag, TAG2[tag], TAG[tag], vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADVERB', 'PUNCT', 'PRONOUN', 'VERB', 'DETERMINER', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Finally , she enquires the day .'\n",
    "sentence = sentence.lower().split()\n",
    "\n",
    "#List of dictionaries so i dont have to remember which POS is which index\n",
    "viterb = [{tag:j for (tag,j) in zip(tags,range(len(tags)))} for i in range(len(sentence))]\n",
    "backp = [{tag:j for (tag,j) in zip(tags,range(len(tags)))} for i in range(len(sentence))]\n",
    "\n",
    "#Calculate the initial probabilities\n",
    "for tag in tags:\n",
    "    if sentence[0] in WORD:\n",
    "        viterb[0][tag] = INIT[tag]*WORD[sentence[0]][tag]\n",
    "    else:\n",
    "        viterb[0][tag] = 0.0\n",
    "    backp[0][tag] = 0\n",
    "    \n",
    "#Calculating the lattice    \n",
    "for t in range(1, len(sentence)):\n",
    "    for tag in tags:\n",
    "        #The backwards probability for POS = P(t-1,s')*P(s'->s)*P(w,s)\n",
    "        if sentence[t] in WORD:\n",
    "            vals = {prev_tag : viterb[t-1][prev_tag] * TAG2[prev_tag][tag] * WORD[sentence[t]][tag] for prev_tag in tags}\n",
    "        else:\n",
    "            vals = {prev_tag : viterb[t-1][prev_tag] * TAG2[prev_tag][tag] * SUFFIX[sentence[t][-2:]][tag] for prev_tag in tags}\n",
    "        viterb[t][tag] = vals[max(vals, key=vals.get)]\n",
    "        backp[t][tag] = max(vals, key=vals.get)\n",
    "\n",
    "prob_seq = []\n",
    "t = len(sentence)-1\n",
    "best_backpath = max(viterb[t], key=viterb[t].get)\n",
    "prob_seq.append(best_backpath)\n",
    "for i in range(0, len(sentence)-1):\n",
    "    t=len(sentence)-i-1\n",
    "    best_backpath = backp[t][best_backpath]\n",
    "    prob_seq.append(best_backpath)\n",
    "prob_seq = prob_seq[::-1]\n",
    "print(prob_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
