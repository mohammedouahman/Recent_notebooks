{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Gradient Boosting Works\n",
    "\n",
    "## 1. A Loss Function\n",
    "\n",
    "GB requires a differentiable loss function. At each step, a weak learner will be trained on the gradient of the previous cumulative model. The gradient is essentially the residual of each predicted value of the previous stacked collection of weak learners. The amount the cost function would be reduced if the predicted value were one unit closer to the true value for each data point.\n",
    "\n",
    "Regression usually uses RMSE, classification usually uses logarithmic loss.\n",
    "\n",
    "## 2. Weak Learner\n",
    "Regression trees are used to predict the residuals of the previous models. In this way, the output of all stacked models can be added together to get a more accurate prediction for harder to predict cases.\n",
    "\n",
    "Each step learns to adjust a predicted value by grouping the hard to miss cases. If values are continuously miss predicted, the weak learner will try to group these values and adjust with higher weights.\n",
    "\n",
    "Weak learners can be constrained by the maximum number of levels, nodes, splits, and leaf nodes. Weak learners must remain weak.\n",
    "\n",
    "## 3. Additive Model\n",
    "\n",
    "Weak learners are added one at a time, trained on the cost-gradient of all previous levels.\n",
    "\n",
    "\n",
    "\n",
    "# Using XGBoost in Python\n",
    "\n",
    "\n",
    "\n",
    "Efficiency gains come drom the DMatrix data structure. \n",
    "\n",
    "### HyperParameters\n",
    "- learning_rate: step size shrinkage fore each new tree added to the model [0,1]\n",
    "- max_depth: maximum number of levels of each individual weak learner\n",
    "- subsample: % of observations used to train each tree\n",
    "- colsample_bytree: % of columns to use in training each subsequent tree.\n",
    "- n_estimatore: # of trees to add to the model\n",
    "- objective: loss function to be trained on\n",
    "\n",
    "\n",
    "- Gamma: Controls whether a node will split based on the expected reduction in loss after a split. High gammma -> Fewer splits\n",
    "- Alpha: L! regularization on leaf weights\n",
    "- Lambda: L2 regularization on leaf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None,\n",
      "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
      "       subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',categories = categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "X_train = vect.fit_transform(newsgroups_train.data)\n",
    "y_train = newsgroups_train.target\n",
    "X_test = vect.transform(newsgroups_test.data)\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "feature_names = np.asarray(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3b064fe869cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1109\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1110\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf.fit(X_train,y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmatrix = xgb.DMatrix(data=X_train,label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "params = {'colsample_bytree': 1,'learning_rate': 0.1,'max_depth': 5}\n",
    "clf = xgb.train(params, dtrain = dmatrix)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances\n",
    "\n",
    "Count the number of times a feature is split on across all boosting rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "xgb.plot_importance(clf,max_num_features=20)\n",
    "plt.rcParams['figure.figsize'] = [12, 12]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to figure out how to sort feature importances and compart to the Vectorizer feature_names object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_params = {'subsample':[0.3,0.5,1],'learning_rate':[0.1,0.5],'max_depth': [3, 5, 7], 'min_child_weight': [1,3,5],}\n",
    "ind_params = {'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "\n",
    "RS = RandomizedSearchCV(xgb.XGBClassifier(**ind_params),\n",
    "                       cv_params,\n",
    "                       scoring = 'accuracy',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)\n",
    "\n",
    "RS.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With XGBoost\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/get_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.387719\ttrain-error:0.31972\n",
      "[1]\teval-error:0.336842\ttrain-error:0.224037\n",
      "[2]\teval-error:0.314035\ttrain-error:0.164527\n",
      "[3]\teval-error:0.3\ttrain-error:0.144691\n",
      "[4]\teval-error:0.27193\ttrain-error:0.11902\n",
      "[5]\teval-error:0.252632\ttrain-error:0.10035\n",
      "[6]\teval-error:0.25614\ttrain-error:0.101517\n",
      "[7]\teval-error:0.268421\ttrain-error:0.089848\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(data=X_train,label=y_train)\n",
    "dval = xgb.DMatrix(data=X_val,label=y_val)\n",
    "dtest = xgb.DMatrix(data=X_test,label=y_test)\n",
    "param = {'max_depth':2,\n",
    "         'eta':1,\n",
    "         'silent':True,\n",
    "         'objective':'binary:logistic' }\n",
    "\n",
    "# specify validations set to watch performance\n",
    "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "bst = xgb.train(param,dtrain,\n",
    "                evals=watchlist,\n",
    "                num_boost_round=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest)\n",
    "bst.save_model('model0001.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst2 = xgb.Booster(model_file='model0001.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = bst2.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('xgb.pkl','wb') as outfile:\n",
    "    pickle.dump(bst2,outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = xgb.cv(param, dtrain, num_boost_round=10,nfold=5)\n",
    "\n",
    "\"\"\"\n",
    "params : dict\n",
    "    Booster params.\n",
    "dtrain : DMatrix \n",
    "    Data to be trained.\n",
    "num_boost_round : int\n",
    "    Number of boosting iterations.\n",
    "nfold : int\n",
    "    Number of folds in CV.\n",
    "stratified : bool\n",
    "    Perform stratified sampling.\n",
    "folds : a KFold or StratifiedKFold instance or list of fold indices\n",
    "    Sklearn KFolds or StratifiedKFolds object.\n",
    "    Alternatively may explicitly pass sample indices for each fold.\n",
    "    For ``n`` folds, **folds** should be a length ``n`` list of tuples.\n",
    "    Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used\n",
    "    as the training samples for the ``n`` th fold and ``out`` is a list of\n",
    "    indices to be used as the testing samples for the ``n`` th fold.\n",
    "metrics : string or list of strings\n",
    "    Evaluation metrics to be watched in CV.\n",
    "obj : function\n",
    "    Custom objective function.\n",
    "feval : function\n",
    "    Custom evaluation function.\n",
    "maximize : bool\n",
    "    Whether to maximize feval.\n",
    "early_stopping_rounds: int\n",
    "    Activates early stopping. CV error needs to decrease at least\n",
    "    every <early_stopping_rounds> round(s) to continue.\n",
    "    Last entry in evaluation history is the one from best iteration.\n",
    "fpreproc : function\n",
    "    Preprocessing function that takes (dtrain, dtest, param) and returns\n",
    "    transformed versions of those.\n",
    "as_pandas : bool, default True\n",
    "    Return pd.DataFrame when pandas is installed.\n",
    "    If False or pandas is not installed, return np.ndarray\n",
    "verbose_eval : bool, int, or None, default None\n",
    "    Whether to display the progress. If None, progress will be displayed\n",
    "    when np.ndarray is returned. If True, progress will be displayed at\n",
    "    boosting stage. If an integer is given, progress will be displayed\n",
    "    at every given `verbose_eval` boosting stage.\n",
    "show_stdv : bool, default True\n",
    "    Whether to display the standard deviation in progress.\n",
    "    Results are not affected, and always contains std.\n",
    "seed : int\n",
    "    Seed used to generate the folds (passed to numpy.random.seed).\n",
    "callbacks : list of callback functions\n",
    "    List of callback functions that are applied at end of each iteration.\n",
    "    It is possible to use predefined callbacks by using\n",
    "    :ref:`Callback API <callback_api>`.\n",
    "    Example:\n",
    "\n",
    "    .. code-block:: python\"\"\"\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-error-mean  train-error-std  test-error-mean  test-error-std\n",
      "0          0.319135         0.006207         0.324371        0.025551\n",
      "1          0.226660         0.007444         0.275303        0.038737\n",
      "2          0.179700         0.022443         0.240303        0.041859\n",
      "3          0.154903         0.025005         0.213471        0.040183\n",
      "4          0.130982         0.023309         0.202972        0.039993\n",
      "5          0.110263         0.016434         0.180851        0.036152\n",
      "6          0.098018         0.011061         0.184285        0.038796\n",
      "7          0.088099         0.010866         0.176139        0.035412\n",
      "8          0.079349         0.012557         0.166837        0.028136\n",
      "9          0.069428         0.008737         0.155195        0.027340\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.273209549071618"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = dtrain.get_label()\n",
    "ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.715500</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.708998</td>\n",
       "      <td>0.024066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.817305</td>\n",
       "      <td>0.013760</td>\n",
       "      <td>0.767994</td>\n",
       "      <td>0.027433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
       "0        0.715500       0.005518       0.708998      0.024066\n",
       "1        0.817305       0.013760       0.767994      0.027433"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param['scale_pos_weight']=ratio\n",
    "xgb.cv(param, dtrain, 2, nfold=5,\n",
    "       metrics={'auc'}, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
